{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"project-nlp","language":"python","name":"project-nlp"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"USE_disaster_project.ipynb adlı not defterinin kopyası","provenance":[{"file_id":"https://github.com/Ahmetemintek/tweet_classification/blob/master/USE_disaster_project.ipynb","timestamp":1634119184155}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"EwhKiPQ29qQ4"},"source":["# Disaster Tweets Classification By Using SparkNLP\n","In this project, I built a classification model that predicts which Tweets are about real disasters and which one’s aren’t. <br/>\n","I have accessed to a dataset that contains 10,000 tweets. "]},{"cell_type":"code","metadata":{"id":"bQGjOe6O-DrN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117228497,"user_tz":-180,"elapsed":95507,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"cc786814-a59d-4001-e089-88e47dfa95de"},"source":["!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/colab_setup.sh -O - | bash"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-13 09:25:33--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/colab_setup.sh\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1608 (1.6K) [text/plain]\n","Saving to: ‘STDOUT’\n","\n","\r-                     0%[                    ]       0  --.-KB/s               \r-                   100%[===================>]   1.57K  --.-KB/s    in 0s      \n","\n","2021-10-13 09:25:33 (31.8 MB/s) - written to stdout [1608/1608]\n","\n","setup Colab for PySpark 3.0.2 and Spark NLP 3.1.0\n","Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [607 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,431 kB]\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,367 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,210 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [640 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,803 kB]\n","Fetched 10.3 MB in 3s (3,121 kB/s)\n","Reading package lists... Done\n","tar: spark-3.0.2-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","\u001b[K     |████████████████████████████████| 204.8 MB 55 kB/s \n","\u001b[K     |████████████████████████████████| 44 kB 1.6 MB/s \n","\u001b[K     |████████████████████████████████| 198 kB 41.3 MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"CiXvXUbk9qQ9"},"source":["#Importing Useful Packages\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","import sparknlp\n","from sparknlp.annotator import *\n","from sparknlp.base import *\n","import pyspark.sql.functions as F\n","from pyspark.sql.functions import col\n","from pyspark.ml import Pipeline\n","from sparknlp.pretrained import PretrainedPipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETMjf6Ys9qQ_"},"source":["#Starting Sparknlp\n","spark= sparknlp.start()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgyaOyHS9qQ_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117341242,"user_tz":-180,"elapsed":230,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"2cd54c12-c43b-4ae1-874e-e5d0ad7b8b52"},"source":["print(\"SparkNLP version: {}\".format(sparknlp.version()))\n","print(\"Pyspark version: {}\".format(spark.version))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SparkNLP version: 3.1.0\n","Pyspark version: 3.0.2\n"]}]},{"cell_type":"code","metadata":{"id":"MM-2hJ4D9qRB"},"source":["# Loading train and test datasets\n","df_train= spark.read\\\n","    .option(\"header\", True)\\\n","    .csv(\"train.csv\")\n","df_test= spark.read\\\n","    .option(\"header\", True)\\\n","    .csv(\"test.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylAvW7zU9qRC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117422645,"user_tz":-180,"elapsed":615,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"573b4a78-968d-4b6c-89be-3b3438e2c165"},"source":["df_train.show(5, truncate=False)  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+--------+-------------------------------------------------------------------------------------------------------------------------------------+------+\n","|id |keyword|location|text                                                                                                                                 |target|\n","+---+-------+--------+-------------------------------------------------------------------------------------------------------------------------------------+------+\n","|1  |null   |null    |Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                |1     |\n","|4  |null   |null    |Forest fire near La Ronge Sask. Canada                                                                                               |1     |\n","|5  |null   |null    |All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected|1     |\n","|6  |null   |null    |13,000 people receive #wildfires evacuation orders in California                                                                     |1     |\n","|7  |null   |null    |Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                              |1     |\n","+---+-------+--------+-------------------------------------------------------------------------------------------------------------------------------------+------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"H9CwK99T9qRD"},"source":["**id:** This column consist ids per each tweets <br/>\n","**keyword:** A keyword from that tweet (although this may be blank) <br/>\n","**location:** The location the tweet was sent from (may also be blank) <br/>\n","**text:** The text of a tweet <br/>\n","**target:** In train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0) <br/>"]},{"cell_type":"code","metadata":{"id":"FlOwfG6m9qRD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117429724,"user_tz":-180,"elapsed":4083,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"1763e34e-bab4-42f2-8f71-adf48e624122"},"source":["df_train.groupby(\"target\")\\\n","    .count()\\\n","    .orderBy(col(\"count\")).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+-----+\n","|target|count|\n","+------+-----+\n","|  null| 1211|\n","|     1| 3081|\n","|     0| 4095|\n","+------+-----+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ofg6IyMI9qRD"},"source":["Data include some null values. I will drop them. Also, firstly I will drop keyword and location columns."]},{"cell_type":"code","metadata":{"id":"77GhkGPu9qRE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117444894,"user_tz":-180,"elapsed":573,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"5e875e52-93e1-4094-de42-36a1c717943c"},"source":["drop_col= [\"keyword\", \"location\"]\n","df_train= df_train.drop(*drop_col)\n","df_train.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+--------------------+------+\n","| id|                text|target|\n","+---+--------------------+------+\n","|  1|Our Deeds are the...|     1|\n","|  4|Forest fire near ...|     1|\n","|  5|All residents ask...|     1|\n","|  6|13,000 people rec...|     1|\n","|  7|Just got sent thi...|     1|\n","|  8|#RockyFire Update...|     1|\n","| 10|#flood #disaster ...|     1|\n","| 13|I'm on top of the...|     1|\n","| 14|There's an emerge...|     1|\n","| 15|I'm afraid that t...|     1|\n","| 16|Three people died...|     1|\n","| 17|Haha South Tampa ...|     1|\n","| 18|#raining #floodin...|     1|\n","| 19|#Flood in Bago My...|     1|\n","| 20|Damage to school ...|     1|\n","| 23|      What's up man?|     0|\n","| 24|       I love fruits|     0|\n","| 25|    Summer is lovely|     0|\n","| 26|   My car is so fast|     0|\n","| 28|What a goooooooaa...|     0|\n","+---+--------------------+------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"rUC-uhao9qRE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117451332,"user_tz":-180,"elapsed":2369,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"9e5e4f5d-6afa-4e40-a4fb-47b100706b93"},"source":["#Dropping the values which is null in the target column.\n","df_train= df_train.na.drop(how=\"any\")\n","df_train.groupby(\"target\")\\\n","    .count()\\\n","    .orderBy(col(\"count\")).show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+-----+\n","|target|count|\n","+------+-----+\n","|     1| 3081|\n","|     0| 4095|\n","+------+-----+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"MIwiLxcT9qRF"},"source":["Now, there aren't any null values. <br/>\n","Firstly I will apply SparkNLP DocumentAssembler. DocumentAssembler is a entry point to SparkNLP pipeline. <br/>\n","After thar, I will apply Universal Sentence Encoder and then create ClassifierDL. <br/>\n","Finally I will put into the pipeline and fit with the train_set."]},{"cell_type":"code","metadata":{"id":"brxvdQYR9qRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117566390,"user_tz":-180,"elapsed":110842,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"85af2cb9-a8bd-4422-9672-1a7a6d0efe43"},"source":["document = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","use = UniversalSentenceEncoder.pretrained()\\\n"," .setInputCols([\"document\"])\\\n"," .setOutputCol(\"sentence_embeddings\")\n","\n","classsifierdl = ClassifierDLApproach()\\\n","    .setInputCols([\"sentence_embeddings\"])\\\n","    .setOutputCol(\"class\")\\\n","    .setLabelColumn(\"target\")\\\n","    .setMaxEpochs(10)\\\n","    .setEnableOutputLogs(True)\\\n","    .setLr(0.004)\\\n","\n","nlpPipeline = Pipeline(\n","    stages = [\n","        document,\n","        use,\n","        classsifierdl\n","    ])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n"]}]},{"cell_type":"code","metadata":{"id":"t8Z_DWH_9qRH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117590659,"user_tz":-180,"elapsed":1264,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"2a13fa21-d218-429b-c1e0-a585112b609f"},"source":["#splitting the data into train set and test set\n","(train_set, test_set)= df_train.randomSplit([0.8, 0.2], seed=100)\n","print(\"Train set shape: {}\".format((train_set.count(), len(train_set.columns))))\n","print(\"Test set shape: {}\".format((test_set.count(), len(test_set.columns))))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set shape: (5744, 3)\n","Test set shape: (1432, 3)\n"]}]},{"cell_type":"code","metadata":{"id":"3kyu4NrT9qRJ"},"source":["#fitting with train_set\n","use_model = nlpPipeline.fit(train_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EF8-RNZ59qRR"},"source":["When we fit pipeline, Spark NLP will write the training logs to \"annotator_logs\" folder in our home directory. <br/>\n","Here is how you can read the logs:"]},{"cell_type":"code","metadata":{"id":"TYXGD_1B9qRS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117636028,"user_tz":-180,"elapsed":220,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"c043f083-ec71-452c-8cb0-0683a16b23b5"},"source":["!cd ~/annotator_logs && ls -l"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 4\n","-rw-r--r-- 1 root root 788 Oct 13 09:33 ClassifierDLApproach_f70d46c11503.log\n"]}]},{"cell_type":"markdown","metadata":{"id":"PtzH7vig9qRT"},"source":["For check the result of our model:"]},{"cell_type":"code","metadata":{"id":"EbaBBKbM9qRT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117651269,"user_tz":-180,"elapsed":225,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"020d1f52-23a4-4197-b222-0aebb4979bfc"},"source":["!cat ~/annotator_logs/ClassifierDLApproach_f70d46c11503.log"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training started - epochs: 10 - learning_rate: 0.004 - batch_size: 64 - training_examples: 5744 - classes: 2\n","Epoch 0/10 - 1.65s - loss: 42.825516 - acc: 0.81115407 - batches: 90\n","Epoch 1/10 - 1.37s - loss: 39.700607 - acc: 0.8449789 - batches: 90\n","Epoch 2/10 - 1.65s - loss: 37.751377 - acc: 0.85750234 - batches: 90\n","Epoch 3/10 - 1.36s - loss: 37.139404 - acc: 0.8644077 - batches: 90\n","Epoch 4/10 - 1.39s - loss: 36.602478 - acc: 0.8703768 - batches: 90\n","Epoch 5/10 - 1.37s - loss: 36.50698 - acc: 0.8719569 - batches: 90\n","Epoch 6/10 - 1.34s - loss: 36.32841 - acc: 0.87845266 - batches: 90\n","Epoch 7/10 - 1.45s - loss: 36.23835 - acc: 0.88126165 - batches: 90\n","Epoch 8/10 - 1.41s - loss: 36.23583 - acc: 0.88448036 - batches: 90\n","Epoch 9/10 - 1.35s - loss: 36.2266 - acc: 0.8860604 - batches: 90\n"]}]},{"cell_type":"markdown","metadata":{"id":"5VeOlKiW9qRT"},"source":["We achieved %87 accuracy score on train_set. <br/>\n","Let's check the model with test_set by using sklearn metrics."]},{"cell_type":"code","metadata":{"id":"yjdfk4j79qRU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117679464,"user_tz":-180,"elapsed":2936,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"24843075-77e3-4c37-ebf2-acaf100c723d"},"source":["preds= use_model.transform(test_set)\n","preds.select(\"target\", \"text\", \"class.result\").show(5, truncate=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+----------------------------------------------------------------------------------------------------------------------------------------+------+\n","|target|text                                                                                                                                    |result|\n","+------+----------------------------------------------------------------------------------------------------------------------------------------+------+\n","|0     |I liked a @YouTube video http://t.co/0h7OUa1pns Call of Duty: Ghosts - Campanha - EP 6 'Tsunami'                                        |[0]   |\n","|1     |Gail and Russell saw lots of hail at their Dalroy home - they have video of twister 1/2 mile from their home #yyc http://t.co/3VfKEdGrsO|[1]   |\n","|0     |love 106.1 The Twister @1061thetwister  and Maddie and Tae #OKTXDUO                                                                     |[0]   |\n","|0     |Brain twister let drop up telly structuring cast: EDcXO                                                                                 |[0]   |\n","|0     |This is my jam: Riser by Dierks Bentley @1061TheTwister ? #iHeartRadio #NowPlaying http://t.co/zQoScQD64h http://t.co/yLvVF139BB        |[0]   |\n","+------+----------------------------------------------------------------------------------------------------------------------------------------+------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"VInBfR_89qRU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117741725,"user_tz":-180,"elapsed":4374,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"511d265f-b85e-485f-d959-9280ff1162eb"},"source":["df= use_model.transform(test_set).select(\"target\", \"document\", \"class.result\").toPandas()\n","df[\"result\"]= df[\"result\"].apply(lambda x: x[0])\n","print(classification_report(df[\"target\"], df[\"result\"]))\n","print(accuracy_score(df[\"target\"], df[\"result\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.80      0.89      0.84       813\n","           1       0.83      0.70      0.76       619\n","\n","    accuracy                           0.81      1432\n","   macro avg       0.81      0.80      0.80      1432\n","weighted avg       0.81      0.81      0.81      1432\n","\n","0.8079608938547486\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bu54vhYy9qRU"},"source":["We achieved %81 accuracy score on test_set as well. </br>\n","####  Applying the model on the test.csv data for submission."]},{"cell_type":"code","metadata":{"id":"U-rLCMH39qRV"},"source":["df_test= spark.read\\\n","    .option(\"header\", True)\\\n","    .csv(\"test.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsAtYBuV9qRV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117755261,"user_tz":-180,"elapsed":225,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"2c99dd79-a564-430b-9641-1ca336bf44a2"},"source":["df_test.show(5, truncate=False)  #this is the test data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+--------+------------------------------------------------------------------------------------------------+\n","|id |keyword|location|text                                                                                            |\n","+---+-------+--------+------------------------------------------------------------------------------------------------+\n","|0  |null   |null    |Just happened a terrible car crash                                                              |\n","|2  |null   |null    |Heard about #earthquake is different cities, stay safe everyone.                                |\n","|3  |null   |null    |there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all|\n","|9  |null   |null    |Apocalypse lighting. #Spokane #wildfires                                                        |\n","|11 |null   |null    |Typhoon Soudelor kills 28 in China and Taiwan                                                   |\n","+---+-------+--------+------------------------------------------------------------------------------------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"9ZF1qWAE9qRW","outputId":"9a9507cd-83aa-4a11-8542-af8613265aa9"},"source":["submission.show(5, truncate=False) #this is the submission format"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["+---+------+\n","|id |target|\n","+---+------+\n","|0  |0     |\n","|2  |0     |\n","|3  |0     |\n","|9  |0     |\n","|11 |0     |\n","+---+------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"8CtgdfP59qRW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117769557,"user_tz":-180,"elapsed":569,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"13b176c1-9335-4d18-d859-47041ca06fb9"},"source":["preds= use_model.transform(df_test)\n","preds.select(\"id\",\"text\", \"class.result\").show(5, truncate=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------------------------------------------------------------------------------------+------+\n","|id |text                                                                                            |result|\n","+---+------------------------------------------------------------------------------------------------+------+\n","|0  |Just happened a terrible car crash                                                              |[1]   |\n","|2  |Heard about #earthquake is different cities, stay safe everyone.                                |[1]   |\n","|3  |there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all|[1]   |\n","|9  |Apocalypse lighting. #Spokane #wildfires                                                        |[1]   |\n","|11 |Typhoon Soudelor kills 28 in China and Taiwan                                                   |[1]   |\n","+---+------------------------------------------------------------------------------------------------+------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"OoyAuZ6W9qRX"},"source":["final = use_model.transform(df_test).select(\"id\", \"class.result\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9wug5tW9qRX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117785075,"user_tz":-180,"elapsed":358,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"e0551084-c355-4fdc-8d30-edf1bdf8f519"},"source":["# This is the final step\n","final= final.withColumnRenamed(\"result\" ,\"target\")\n","final.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------+\n","| id|target|\n","+---+------+\n","|  0|   [1]|\n","|  2|   [1]|\n","|  3|   [1]|\n","|  9|   [1]|\n","| 11|   [1]|\n","+---+------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"WVMRD2OABGoO"},"source":["final= final.toPandas()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMAVwYgX9qRX"},"source":["final.to_csv(index=False, path_or_buf=\"/content/final_submission.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTyHlAP0BDuZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634117957450,"user_tz":-180,"elapsed":306,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"2afdc960-8fe6-4335-ffc5-a826514d217e"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["id        3613\n","target    3613\n","dtype: int64"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"0lnfvflNBTO-"},"source":[""],"execution_count":null,"outputs":[]}]}
